{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1hnoa-SAkGCeSENfgMuwBJuNXXRhOh0P2",
      "authorship_tag": "ABX9TyMLNJpFEactmgQ4E00MDsRU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bigboyfreezy/sign-language-detection/blob/main/SignLanguageDetector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SIGN LANGUAGE DETECTION WITH KERAS\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Detecting Sign Language Letters in Real Time Using MediaPipe and Keras**\n"
      ],
      "metadata": {
        "id": "KCAbo7rxBN5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA PREPARATION FOR TRAINING**\n",
        "1. **Matplotlib** - is a comprehensive library for creating static, animated, and interactive visualizations in Python.\n",
        "2. **Keras** - a high-level neural networks API running on top of TensorFlow.\n",
        "3. Sequential class  - from Keras, which is a linear stack of layers for building neural networks layer by layer\n"
      ],
      "metadata": {
        "id": "0oEruHfECA9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "#Imports various layers from Keras that are commonly used in convolutional neural networks (CNNs) for image classification tasks.\n",
        "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout , BatchNormalization\n",
        "#helps generate augmented images by applying various transformations to the input data during training.\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# function from scikit-learn, which is used to split the dataset into training and testing sets.\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Imports functions for generating a classification report and confusion matrix, which are useful for evaluating the performance of a classification model.\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "# callback from Keras, which dynamically adjusts the learning rate during training based on a specified condition, typically used to improve training performance.\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "KSMN4dO8BPrG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre Processing Key Points**\n",
        "1. Normalization - In digital images, pixel values are typically represented as integers in the range [0, 255], where 0 corresponds to black and 255 corresponds to white. Normalizing by dividing by 255 scales these values to be in the range [0, 1]. This is a common preprocessing step for image data when working with neural networks. Normalization helps in stabilizing and accelerating the training process, making it easier for the optimization algorithm to converge.\n",
        "\n",
        "2. In deep learning models, especially convolutional neural networks (CNNs), the input data is often required to have a specific shape. The common shape for image data is a 4D tensor with dimensions (number of samples, height, width, channels).\n",
        "\n",
        "- The parameter -1 in the reshape operation is used to infer the number of samples automatically. It is a placeholder for the size of the first dimension, and the actual size is calculated based on the size of the remaining dimensions\n",
        "- 28, 28 represents the height and width of the images, assuming they are 28x28 pixels.\n",
        "- 1 is the number of channels. In this case, the images are assumed to be grayscale, so there is only one channel. For color images with RGB channels, this value would be 3."
      ],
      "metadata": {
        "id": "P8elWgDpL7q0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt:\n",
        "\n",
        "# Read the training and testing dataset from  CSV file\n",
        "train= pd.read_csv(\"/content/drive/MyDrive/sign_mnist_train/sign_mnist_train.csv\")\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/sign_mnist_test/sign_mnist_test.csv\")\n",
        "\n",
        "#  Extracts the labels (target variable) from the training and testing dataset and assigns them to the variable x_train and y_train.\n",
        "y_train = train['label']\n",
        "y_test = test['label']\n",
        "# Removes the 'label' column from both the training and testing DataFrames\n",
        "del train['label']\n",
        "del test['label']\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "label_binarizer = LabelBinarizer()\n",
        "y_train = label_binarizer.fit_transform(y_train)\n",
        "y_test = label_binarizer.fit_transform(y_test)\n",
        "\n",
        "x_train = train.values\n",
        "x_test = test.values\n",
        "\n",
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "x_train = x_train.reshape(-1,28,28,1)\n",
        "x_test = x_test.reshape(-1,28,28,1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WVmG54jmHGjH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BreakDown Of ImageDatagenerator**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "ImageDataGenerator for data augmentation and defining a learning rate reduction callback. Let's break it down:\n",
        "\n",
        "- datagen = ImageDataGenerator(...): Creates an instance of ImageDataGenerator with various configuration options for data augmentation. This generator will be used to generate augmented images during training.\n",
        "\n",
        "- featurewise_center, samplewise_center, featurewise_std_normalization, and samplewise_std_normalization: These options control whether to center the mean and normalize the standard deviation of the input data. In this case, they are set to False.\n",
        "\n",
        "- zca_whitening: ZCA whitening is a form of data preprocessing that can enhance the effectiveness of the model by decorrelating the features. Here, it is set to False.\n",
        "\n",
        "- rotation_range: Randomly rotates images in the range specified (degrees, 0 to 180). It is set to 10 degrees.\n",
        "\n",
        "- zoom_range: Randomly zooms into the image. It is set to 0.1, meaning a maximum zoom of 10%.\n",
        "\n",
        "- width_shift_range and height_shift_range: Randomly shifts images horizontally and vertically by a fraction of the total width and height, respectively. Both are set to 0.1, allowing for a maximum shift of 10%.\n",
        "\n",
        "- horizontal_flip and vertical_flip: Randomly flip images horizontally and vertically. Both are set to False, meaning no flipping is applied.\n",
        "\n",
        "- datagen.fit(x_train): Fits the ImageDataGenerator on the training data (x_train). This calculates the necessary statistics (e.g., mean and standard deviation) needed for data augmentation based on the provided configuration.\n",
        "\n",
        "**learning_rate_reduction**\n",
        "\n",
        "\n",
        "---\n",
        "Creates an instance of ReduceLROnPlateau callback.\n",
        "\n",
        "\n",
        "- monitor='val_accuracy': Monitors the validation accuracy during training.\n",
        "\n",
        "- patience=2: Number of epochs with no improvement after which learning rate will be reduced.\n",
        "\n",
        "- verbose=1: Prints a message when the learning rate is reduced.\n",
        "\n",
        "- factor=0.5: Factor by which the learning rate will be reduced. In this case, it is reduced by half.\n",
        "\n",
        "- min_lr=0.00001: Lower bound on the learning rate."
      ],
      "metadata": {
        "id": "zOt1_7TNOSqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.1, # Randomly zoom image\n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=False,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "datagen.fit(x_train)\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)"
      ],
      "metadata": {
        "id": "rHjfWUckOTAd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layer Breakdown**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "***Sequential***\n",
        "- model = Sequential(): Initializes a sequential model.\n",
        "\n",
        "***Convolutional Layer 1***:\n",
        "- 75 filters with a kernel size of (3,3).\n",
        "- strides=1: The step size of the convolution operation.\n",
        "- padding='same': Pads the input such that the output has the same height and width.\n",
        "- activation='relu': Applies the Rectified Linear Unit (ReLU) activation function.\n",
        "- input_shape=(28,28,1): Input shape of each image (height, width, channels), assuming grayscale images.\n",
        "\n",
        "***Batch Normalization:***\n",
        "- Normalizes the activations of the previous layer.\n",
        "\n",
        "***MaxPooling Layer 1:***\n",
        "\n",
        "- Performs max pooling with a pool size of (2,2).\n",
        "- strides=2: The step size of the pooling operation.\n",
        "- padding='same': Pads the input such that the output has the same height and width.\n",
        "\n",
        "***Flatten Layer:***\n",
        "- Flattens the input to a one-dimensional array.\n",
        "\n",
        "***Fully Connected (Dense) Layer:***\n",
        "- Fully connected layer with 512 units and ReLU activation.\n",
        "\n",
        "***Dropout Layer:***\n",
        "- Dropout(0.3): Applies dropout with a dropout rate of 0.3 to prevent overfitting.\n",
        "\n",
        "***Output Layer:***\n",
        "- Fully connected layer with 24 units (assuming 24 classes) and softmax activation for multi-class classification."
      ],
      "metadata": {
        "id": "F_sly_-ZPVLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(75 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu' , input_shape = (28,28,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
        "model.add(Conv2D(50 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
        "model.add(Conv2D(25 , (3,3) , strides = 1 , padding = 'same' , activation = 'relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D((2,2) , strides = 2 , padding = 'same'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units = 512 , activation = 'relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(units = 24 , activation = 'softmax'))"
      ],
      "metadata": {
        "id": "AHTq3Hh6PS0b"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Breakdown**\n",
        "\n",
        "---\n",
        "\n",
        "***Model.compile:***\n",
        "- optimizer='adam': Adam optimization algorithm is used as the optimizer. Adam is an adaptive learning rate optimization algorithm that is widely used in deep learning.\n",
        "\n",
        "- loss='categorical_crossentropy': Categorical crossentropy is the loss function used for multi-class classification problems. Since you used one-hot encoded labels, categorical crossentropy is an appropriate choice.\n",
        "\n",
        "- metrics=['accuracy']: The model's performance will be evaluated based on accuracy during training.\n",
        "\n",
        "***Training The Model***\n",
        "- datagen.flow(x_train, y_train, batch_size=128): Generates augmented batches of training data using the previously defined ImageDataGenerator.\n",
        "\n",
        "- epochs=20: Number of epochs for training.\n",
        "\n",
        "- validation_data=(x_test, y_test): Validation data to be used during training.\n",
        "\n",
        "- callbacks=[learning_rate_reduction]: Uses the learning rate reduction callback during training."
      ],
      "metadata": {
        "id": "UJ4e6C9GSwpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(datagen.flow(x_train,y_train, batch_size = 128) ,epochs = 15 , validation_data = (x_test, y_test) , callbacks = [learning_rate_reduction])\n",
        "\n",
        "model.save('signlangmod.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJySM8ZoSw6N",
        "outputId": "92f96a24-0d38-4c78-ffbe-35e2a81fdc0d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 75)        750       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 28, 28, 75)        300       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 14, 14, 75)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 14, 14, 50)        33800     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 14, 14, 50)        0         \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 14, 14, 50)        200       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 50)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 7, 7, 25)          11275     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 7, 7, 25)          100       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 25)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 400)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               205312    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 24)                12312     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 264049 (1.01 MB)\n",
            "Trainable params: 263749 (1.01 MB)\n",
            "Non-trainable params: 300 (1.17 KB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/15\n",
            "215/215 [==============================] - 118s 534ms/step - loss: 0.4471 - accuracy: 0.8503 - val_loss: 5.8952 - val_accuracy: 0.0937 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "215/215 [==============================] - 106s 492ms/step - loss: 0.1347 - accuracy: 0.9565 - val_loss: 0.3680 - val_accuracy: 0.8911 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "215/215 [==============================] - 101s 470ms/step - loss: 0.0703 - accuracy: 0.9779 - val_loss: 0.0557 - val_accuracy: 0.9854 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "215/215 [==============================] - 120s 559ms/step - loss: 0.0520 - accuracy: 0.9826 - val_loss: 0.1281 - val_accuracy: 0.9639 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9881\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "215/215 [==============================] - 132s 614ms/step - loss: 0.0378 - accuracy: 0.9881 - val_loss: 0.1054 - val_accuracy: 0.9578 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "215/215 [==============================] - 106s 490ms/step - loss: 0.0240 - accuracy: 0.9928 - val_loss: 0.0353 - val_accuracy: 0.9856 - lr: 5.0000e-04\n",
            "Epoch 7/15\n",
            "215/215 [==============================] - 99s 460ms/step - loss: 0.0161 - accuracy: 0.9954 - val_loss: 0.0107 - val_accuracy: 0.9982 - lr: 5.0000e-04\n",
            "Epoch 8/15\n",
            "215/215 [==============================] - 99s 461ms/step - loss: 0.0140 - accuracy: 0.9958 - val_loss: 0.0093 - val_accuracy: 0.9980 - lr: 5.0000e-04\n",
            "Epoch 9/15\n",
            "215/215 [==============================] - ETA: 0s - loss: 0.0114 - accuracy: 0.9968\n",
            "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "215/215 [==============================] - 101s 470ms/step - loss: 0.0114 - accuracy: 0.9968 - val_loss: 0.0111 - val_accuracy: 0.9960 - lr: 5.0000e-04\n",
            "Epoch 10/15\n",
            "215/215 [==============================] - 99s 460ms/step - loss: 0.0094 - accuracy: 0.9976 - val_loss: 0.0393 - val_accuracy: 0.9858 - lr: 2.5000e-04\n",
            "Epoch 11/15\n",
            "215/215 [==============================] - 102s 472ms/step - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.0029 - val_accuracy: 0.9997 - lr: 2.5000e-04\n",
            "Epoch 12/15\n",
            "215/215 [==============================] - 99s 459ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.0028 - val_accuracy: 0.9999 - lr: 2.5000e-04\n",
            "Epoch 13/15\n",
            "215/215 [==============================] - 103s 477ms/step - loss: 0.0071 - accuracy: 0.9980 - val_loss: 0.0020 - val_accuracy: 0.9999 - lr: 2.5000e-04\n",
            "Epoch 14/15\n",
            "215/215 [==============================] - 111s 517ms/step - loss: 0.0055 - accuracy: 0.9989 - val_loss: 7.4209e-04 - val_accuracy: 1.0000 - lr: 2.5000e-04\n",
            "Epoch 15/15\n",
            "215/215 [==============================] - 102s 476ms/step - loss: 0.0067 - accuracy: 0.9980 - val_loss: 0.0101 - val_accuracy: 0.9976 - lr: 2.5000e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part Two: Implementing the Model\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "KiXIIcFJUmZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. First We install the TensorFlow library, which is commonly used for machine learning and deep learning tasks.\n",
        "2. OpenCV library - Used in image processing and performing computer vision tasks. It is an open-source library that can be used to perform tasks like face detection, objection tracking, landmark detection, and much more\n",
        "3. Mediapipe library, a library by Google that simplifies the development of applications for detecting and tracking various keypoints on the human body.\n",
        "4.  Python os module, which provides a way of using operating system-dependent functionality, like reading or writing to the file system.\n",
        "5.  NumPy library, which provides support for large, multi-dimensional arrays and matrices, along with mathematical functions.\n",
        "6.  Pandas library, a powerful data manipulation and analysis library."
      ],
      "metadata": {
        "id": "dRhQG2_u89y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install mediapipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR1nQNMuciPu",
        "outputId": "d7a4f490-8309-487e-e7e2-36bd97b38862"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mediapipe in /root/.local/lib/python3.10/site-packages (0.10.9)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (23.5.26)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.23.5)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.8.0.76)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.20.3)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /root/.local/lib/python3.10/site-packages (from mediapipe) (0.4.6)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8Y4fwZn8EUF"
      },
      "outputs": [],
      "source": [
        "! pip install tensorflow\n",
        "! pip install opencv-python\n",
        "!pip install mediapipe\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "\n",
        "from keras.models import load_model #function from the Keras library, which is used for loading pre-trained machine learning models\n",
        "import numpy as np\n",
        "import time #  allows you to measure and control the time taken by various operations.\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We Have:\n",
        "- An instance of the Hands class (hands) for hand tracking.\n",
        "- The drawing_utils module (mp_drawing) for drawing landmarks and connections on images.\n",
        "- A video capture object (cap) that can be used to capture frames from the camera."
      ],
      "metadata": {
        "id": "eKsulvK3ANPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('/content/drive/MyDrive/signlangmod.h5')\n",
        "\n",
        "mphands = mp.solutions.hands\n",
        "hands = mphands.Hands()\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "_, frame = cap.read() #Reads the first frame from the camera.\n",
        "\n",
        "h, w, c = frame.shape # Retrieves the height, width, and number of channels of the captured frame.\n",
        "\n",
        "img_counter = 0 #A counter to keep track of the captured frames.\n",
        "analysisframe = '' # A variable that might be used for processing or analyzing frames.\n",
        "letterpred = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y'] #A list of letters representing the classes for sign language recognition."
      ],
      "metadata": {
        "id": "G5J9aCZ-WX8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    _, frame = cap.read()\n",
        "\n",
        "    k = cv2.waitKey(1)\n",
        "    if k%256 == 27:\n",
        "        # ESC pressed\n",
        "        print(\"Escape hit, closing...\")\n",
        "        break\n",
        "    elif k%256 == 32:\n",
        "        # SPACE pressed\n",
        "        analysisframe = frame\n",
        "        showframe = analysisframe\n",
        "        cv2.imshow(\"Frame\", showframe)\n",
        "        framergbanalysis = cv2.cvtColor(analysisframe, cv2.COLOR_BGR2RGB)\n",
        "        resultanalysis = hands.process(framergbanalysis)\n",
        "        hand_landmarksanalysis = resultanalysis.multi_hand_landmarks\n",
        "        if hand_landmarksanalysis:\n",
        "            for handLMsanalysis in hand_landmarksanalysis:\n",
        "                x_max = 0\n",
        "                y_max = 0\n",
        "                x_min = w\n",
        "                y_min = h\n",
        "                for lmanalysis in handLMsanalysis.landmark:\n",
        "                    x, y = int(lmanalysis.x * w), int(lmanalysis.y * h)\n",
        "                    if x > x_max:\n",
        "                        x_max = x\n",
        "                    if x < x_min:\n",
        "                        x_min = x\n",
        "                    if y > y_max:\n",
        "                        y_max = y\n",
        "                    if y < y_min:\n",
        "                        y_min = y\n",
        "                y_min -= 20\n",
        "                y_max += 20\n",
        "                x_min -= 20\n",
        "                x_max += 20\n",
        "\n",
        "        analysisframe = cv2.cvtColor(analysisframe, cv2.COLOR_BGR2GRAY)\n",
        "        analysisframe = analysisframe[y_min:y_max, x_min:x_max]\n",
        "        analysisframe = cv2.resize(analysisframe,(28,28))\n",
        "\n",
        "\n",
        "        nlist = []\n",
        "        rows,cols = analysisframe.shape\n",
        "        for i in range(rows):\n",
        "            for j in range(cols):\n",
        "                k = analysisframe[i,j]\n",
        "                nlist.append(k)\n",
        "\n",
        "        datan = pd.DataFrame(nlist).T\n",
        "        colname = []\n",
        "        for val in range(784):\n",
        "            colname.append(val)\n",
        "        datan.columns = colname\n",
        "\n",
        "        pixeldata = datan.values\n",
        "        pixeldata = pixeldata / 255\n",
        "        pixeldata = pixeldata.reshape(-1,28,28,1)\n",
        "        prediction = model.predict(pixeldata)\n",
        "        predarray = np.array(prediction[0])\n",
        "        letter_prediction_dict = {letterpred[i]: predarray[i] for i in range(len(letterpred))}\n",
        "        predarrayordered = sorted(predarray, reverse=True)\n",
        "        high1 = predarrayordered[0]\n",
        "        high2 = predarrayordered[1]\n",
        "        high3 = predarrayordered[2]\n",
        "        for key,value in letter_prediction_dict.items():\n",
        "            if value==high1:\n",
        "                print(\"First Predicted Character 1: \", key)\n",
        "                print('Confidence 1: ', 100*value)\n",
        "            elif value==high2:\n",
        "                print(\"Second Predicted Character 2: \", key)\n",
        "                print('Confidence 2: ', 100*value)\n",
        "            elif value==high3:\n",
        "                print(\"Third Predicted Character 3: \", key)\n",
        "                print('Confidence 3: ', 100*value)\n",
        "        time.sleep(5)\n",
        "\n",
        "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    result = hands.process(framergb)\n",
        "    hand_landmarks = result.multi_hand_landmarks\n",
        "    if hand_landmarks:\n",
        "        for handLMs in hand_landmarks:\n",
        "            x_max = 0\n",
        "            y_max = 0\n",
        "            x_min = w\n",
        "            y_min = h\n",
        "            for lm in handLMs.landmark:\n",
        "                x, y = int(lm.x * w), int(lm.y * h)\n",
        "                if x > x_max:\n",
        "                    x_max = x\n",
        "                if x < x_min:\n",
        "                    x_min = x\n",
        "                if y > y_max:\n",
        "                    y_max = y\n",
        "                if y < y_min:\n",
        "                    y_min = y\n",
        "            y_min -= 20\n",
        "            y_max += 20\n",
        "            x_min -= 20\n",
        "            x_max += 20\n",
        "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
        "    cv2.imshow(\"Frame\", frame)\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "rc6WGGcXWmz_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}